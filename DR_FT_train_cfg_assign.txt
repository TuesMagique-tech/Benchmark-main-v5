149:        config.model_path, config.model, config.net, config.dataset, time.strftime("%m-%d-%H-%M-%S")
172:    # print(f"\nModel: {config.model}")
174:    #     config.model,
178:    #     pretrained=config.pretrained,
181:    print(f"\nModel: {config.model}")
199:        config.model,
203:        pretrained=config.pretrained,
301:    base_loss = torch.nn.CrossEntropyLoss(label_smoothing=config.label_smoothing)
313:    if config.decay_exclue_bias:
326:        optimizer = torch.optim.AdamW(optimizer_parameters, lr=config.lr)
328:        optimizer = torch.optim.AdamW(model.parameters(), lr=config.lr)
333:    # total_steps = len(train_dataloader) * config.epochs
334:    # warmup_steps = int(len(train_dataloader) * config.warmup_epochs)
338:    total_steps = steps_per_epoch * config.epochs
339:    warmup_steps = int(steps_per_epoch * config.warmup_epochs)
342:    if config.scheduler == "polynomial":
343:        print(f"\nScheduler: polynomial – max LR: {config.lr} – end LR: {config.lr_end}")
347:            lr_end=config.lr_end,
351:    elif config.scheduler == "cosine":
352:        print(f"\nScheduler: cosine – max LR: {config.lr}")
356:    elif config.scheduler == "constant":
357:        print(f"\nScheduler: constant – max LR: {config.lr}")
364:    print(f"Warmup Epochs: {config.warmup_epochs} – Warmup Steps: {warmup_steps}")
365:    print(f"Train Epochs: {config.epochs} – Total Train Steps: {total_steps}")
391:    for epoch in range(1, config.epochs + 1):
407:        if (epoch % config.eval_every_n_epoch == 0) or (epoch == config.epochs):
